{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\punya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\punya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\punya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: Developing your Zomato business account and profile is a great way to boost your restaurant’s online reputation\n",
      "\n",
      "Top results:\n",
      "1. zomato.txt (Score: 0.207095)\n",
      "2. swiggy.txt (Score: 0.124229)\n",
      "3. instagram.txt (Score: 0.058227)\n",
      "4. messenger.txt (Score: 0.057373)\n",
      "5. youtube.txt (Score: 0.056774)\n",
      "6. Discord.txt (Score: 0.051449)\n",
      "7. bing.txt (Score: 0.049875)\n",
      "8. paypal.txt (Score: 0.046539)\n",
      "9. reddit.txt (Score: 0.042547)\n",
      "10. flipkart.txt (Score: 0.040045)\n",
      "--------------------------------------------------\n",
      "Query 2: Warwickshire, came from an ancient family and was the heiress to some land\n",
      "\n",
      "Top results:\n",
      "1. shakespeare.txt (Score: 0.117581)\n",
      "2. levis.txt (Score: 0.023595)\n",
      "3. Adobe.txt (Score: 0.022267)\n",
      "4. google.txt (Score: 0.020327)\n",
      "5. nike.txt (Score: 0.018433)\n",
      "6. zomato.txt (Score: 0.017084)\n",
      "7. huawei.txt (Score: 0.013440)\n",
      "8. skype.txt (Score: 0.011370)\n",
      "9. blackberry.txt (Score: 0.010775)\n",
      "10. Dell.txt (Score: 0.010586)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Initialize stemmer, lemmatizer, and stop words\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "# Preprocessing function: lowercase, remove punctuation, remove stopwords, apply stemming and lemmatization\n",
    "def preprocess(text):\n",
    "    text = text.lower()  # case folding\n",
    "    text = re.sub(r\"\\W+\", \" \", text)  # remove non-word characters\n",
    "    tokens = text.split()\n",
    "    tokens = [\n",
    "        lemmatizer.lemmatize(stemmer.stem(token))\n",
    "        for token in tokens\n",
    "        if token not in stop_words\n",
    "    ]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Build the inverted index and document lengths\n",
    "def build_index(corpus_dir):\n",
    "    index = defaultdict(list)  # term -> postings (docID, term frequency)\n",
    "    doc_lengths = {}  # docID -> document length (for normalization)\n",
    "    N = 0  # total number of documents\n",
    "    doc_ids = {}  # mapping of filenames to docIDs\n",
    "\n",
    "    for docID, filename in enumerate(os.listdir(corpus_dir), 1):\n",
    "        if filename.endswith(\".txt\"):  # process only text files\n",
    "            N += 1\n",
    "            doc_ids[docID] = filename  # map docID to filename\n",
    "            with open(\n",
    "                os.path.join(corpus_dir, filename), \"r\", encoding=\"utf-8\"\n",
    "            ) as file:\n",
    "                content = file.read()\n",
    "                tokens = preprocess(content)\n",
    "                term_freqs = Counter(tokens)\n",
    "                doc_length = 0\n",
    "\n",
    "                for term, freq in term_freqs.items():\n",
    "                    log_tf = 1 + math.log10(freq)  # log(tf) for documents\n",
    "                    index[term].append((docID, log_tf))\n",
    "                    doc_length += log_tf**2\n",
    "\n",
    "                doc_lengths[docID] = math.sqrt(\n",
    "                    doc_length\n",
    "                )  # store the length for normalization\n",
    "\n",
    "    return index, doc_lengths, N, doc_ids\n",
    "\n",
    "\n",
    "# Calculate the tf-idf for the query\n",
    "def compute_query_tfidf(query, index, N):\n",
    "    query_tokens = preprocess(query)\n",
    "    query_term_freqs = Counter(query_tokens)\n",
    "    query_tfidf = {}\n",
    "    query_length = 0\n",
    "\n",
    "    for term, freq in query_term_freqs.items():\n",
    "        if term in index:\n",
    "            df = len(index[term])\n",
    "            idf = math.log10(N / df)  # log(N/df) for queries\n",
    "            log_tf = 1 + math.log10(freq)  # log(tf) for queries\n",
    "            query_tfidf[term] = log_tf * idf  # tf-idf for query\n",
    "            query_length += (log_tf * idf) ** 2\n",
    "\n",
    "    query_length = math.sqrt(query_length)  # query normalization\n",
    "    return query_tfidf, query_length\n",
    "\n",
    "\n",
    "# Compute cosine similarity between query and documents\n",
    "def cosine_similarity(query_tfidf, query_length, index, doc_lengths):\n",
    "    doc_scores = defaultdict(float)\n",
    "\n",
    "    for term, query_weight in query_tfidf.items():\n",
    "        if term in index:\n",
    "            for docID, doc_weight in index[term]:\n",
    "                doc_scores[docID] += query_weight * doc_weight\n",
    "\n",
    "    # Normalize by document lengths\n",
    "    for docID in doc_scores:\n",
    "        doc_scores[docID] /= query_length * doc_lengths[docID]\n",
    "\n",
    "    return sorted(\n",
    "        doc_scores.items(), key=lambda x: (-x[1], x[0])\n",
    "    )  # sort by score and docID\n",
    "\n",
    "\n",
    "# Search function to process queries and return top 10 relevant documents\n",
    "def search(query, index, doc_lengths, N, doc_ids):\n",
    "    query_tfidf, query_length = compute_query_tfidf(query, index, N)\n",
    "    ranked_docs = cosine_similarity(query_tfidf, query_length, index, doc_lengths)\n",
    "\n",
    "    # Map docIDs back to filenames and return top 10 results\n",
    "    ranked_files = [(doc_ids[docID], score) for docID, score in ranked_docs[:10]]\n",
    "    return ranked_files\n",
    "\n",
    "\n",
    "# Load and index the corpus\n",
    "corpus_dir = \"corpus\"  # change this to your corpus directory\n",
    "index, doc_lengths, N, doc_ids = build_index(corpus_dir)\n",
    "\n",
    "# Test queries\n",
    "queries = [\n",
    "    \"Developing your Zomato business account and profile is a great way to boost your restaurant’s online reputation\",\n",
    "    \"Warwickshire, came from an ancient family and was the heiress to some land\",\n",
    "]\n",
    "\n",
    "for i, query in enumerate(queries, 1):\n",
    "    print(f\"Query {i}: {query}\")\n",
    "    results = search(query, index, doc_lengths, N, doc_ids)\n",
    "    if results:\n",
    "        print(\"\\nTop results:\")\n",
    "        for rank, (filename, score) in enumerate(results, 1):\n",
    "            print(f\"{rank}. {filename} (Score: {score:.6f})\")\n",
    "    else:\n",
    "        print(\"No documents match the query.\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

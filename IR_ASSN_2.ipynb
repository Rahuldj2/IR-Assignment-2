{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNd/0BRsam7QSmuka/nJmb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rahuldj2/IR-Assignment-2/blob/main/IR_ASSN_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0gJFVyO6YWB",
        "outputId": "a9308c06-f7e6-4f88-96a5-a6c8ff7508a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your query (or 'exit' to quit): Developing your Zomato business account and profile is a great way to boost your restaurant’s online reputation\n",
            "\n",
            "Top results:\n",
            "1. zomato.txt (Score: 0.5217394992745741)\n",
            "2. swiggy.txt (Score: 0.30380894525158864)\n",
            "3. instagram.txt (Score: 0.11750733468677174)\n",
            "4. messenger.txt (Score: 0.11032477077510809)\n",
            "5. youtube.txt (Score: 0.09703346075631504)\n",
            "6. HP.txt (Score: 0.08798887157099794)\n",
            "7. reddit.txt (Score: 0.07835569065197673)\n",
            "8. flipkart.txt (Score: 0.06326544408856068)\n",
            "9. Uber.txt (Score: 0.062202918107177846)\n",
            "10. shakespeare.txt (Score: 0.05933212325654604)\n",
            "Enter your query (or 'exit' to quit): Warwickshire, came from an ancient family and was the heiress to some land\n",
            "\n",
            "Top results:\n",
            "1. shakespeare.txt (Score: 0.4632969542538422)\n",
            "2. zomato.txt (Score: 0.056159806661941813)\n",
            "3. levis.txt (Score: 0.049038313030313196)\n",
            "4. Adobe.txt (Score: 0.045391726820548456)\n",
            "5. nike.txt (Score: 0.04371198906774426)\n",
            "6. huawei.txt (Score: 0.03376035555034407)\n",
            "7. Dell.txt (Score: 0.02480023239980295)\n",
            "8. blackberry.txt (Score: 0.016368101569128157)\n",
            "9. skype.txt (Score: 0.015267947501203137)\n",
            "10. reliance.txt (Score: 0.01464847371089507)\n",
            "Enter your query (or 'exit' to quit): exit\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "# Preprocessing: Tokenization and normalization (lowercasing and removing punctuation)\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\W+', ' ', text)  # Remove non-word characters\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "# Building the inverted index and document lengths\n",
        "def build_index(corpus_path):\n",
        "    index = defaultdict(list)\n",
        "    doc_lengths = {}\n",
        "    N = 0  # Total number of documents\n",
        "    doc_ids = {}\n",
        "\n",
        "    for i, filename in enumerate(os.listdir(corpus_path)):\n",
        "        if filename.endswith('.txt'):\n",
        "            N += 1\n",
        "            doc_id = i + 1\n",
        "            doc_ids[filename] = doc_id\n",
        "            with open(os.path.join(corpus_path, filename), 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "                tokens = preprocess(content)\n",
        "                token_counts = Counter(tokens)\n",
        "\n",
        "                # Add terms to the index\n",
        "                for term, freq in token_counts.items():\n",
        "                    index[term].append((doc_id, freq))\n",
        "\n",
        "                # Compute the document length (for normalization later)\n",
        "                length = 0\n",
        "                for term, freq in token_counts.items():\n",
        "                    length += (1 + math.log10(freq))**2\n",
        "                doc_lengths[doc_id] = math.sqrt(length)\n",
        "\n",
        "    return index, doc_lengths, N, doc_ids\n",
        "\n",
        "# Calculating cosine similarity between the query and documents\n",
        "def cosine_similarity(query_vec, doc_vec, doc_lengths):\n",
        "    scores = defaultdict(float)\n",
        "    for term, qtf_idf in query_vec.items():\n",
        "        if term in doc_vec:\n",
        "            for doc_id, dtf_idf in doc_vec[term]:\n",
        "                scores[doc_id] += qtf_idf * dtf_idf\n",
        "\n",
        "    for doc_id in scores:\n",
        "        scores[doc_id] /= doc_lengths[doc_id]  # Normalize by document length\n",
        "\n",
        "    return sorted(scores.items(), key=lambda x: (-x[1], x[0]))\n",
        "\n",
        "# Vector space model with lnc.ltc ranking\n",
        "def vsm_search(query, index, doc_lengths, N, doc_ids):\n",
        "    # Preprocess the query\n",
        "    query_tokens = preprocess(query)\n",
        "    query_freqs = Counter(query_tokens)\n",
        "\n",
        "    # Compute tf-idf for query\n",
        "    query_vec = {}\n",
        "    for term, freq in query_freqs.items():\n",
        "        if term in index:\n",
        "            df = len(index[term])  # Document frequency\n",
        "            idf = math.log10(N / df)\n",
        "            query_vec[term] = (1 + math.log10(freq)) * idf\n",
        "\n",
        "    # Compute tf-idf for documents\n",
        "    doc_vec = defaultdict(list)\n",
        "    for term, postings in index.items():\n",
        "        df = len(postings)\n",
        "        idf = math.log10(N / df)\n",
        "        for doc_id, freq in postings:\n",
        "            tf_idf = (1 + math.log10(freq)) * idf\n",
        "            doc_vec[term].append((doc_id, tf_idf))\n",
        "\n",
        "    # Rank documents by cosine similarity\n",
        "    ranked_docs = cosine_similarity(query_vec, doc_vec, doc_lengths)\n",
        "\n",
        "    # Convert doc IDs back to filenames and return top 10 results\n",
        "    ranked_files = [(list(doc_ids.keys())[list(doc_ids.values()).index(doc_id)], score) for doc_id, score in ranked_docs[:10]]\n",
        "    return ranked_files\n",
        "\n",
        "# Main function to run the search engine\n",
        "def main():\n",
        "    corpus_path = 'corpus'\n",
        "    index, doc_lengths, N, doc_ids = build_index(corpus_path)\n",
        "\n",
        "    while True:\n",
        "        query = input(\"Enter your query (or 'exit' to quit): \").strip()\n",
        "        if query.lower() == 'exit':\n",
        "            break\n",
        "        results = vsm_search(query, index, doc_lengths, N, doc_ids)\n",
        "        if results:\n",
        "            print(\"\\nTop results:\")\n",
        "            for i, (filename, score) in enumerate(results, 1):\n",
        "                print(f\"{i}. {filename} (Score: {score})\")\n",
        "        else:\n",
        "            print(\"No documents match the query.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# for preprocessing steps initializing stemming, lemmatizers etc\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "#IN THIS STEP TEXT IS FIRST CONVERTED TO LOWERCASE THEN LEMMATIZATION IS APPLIED AFTER REMOVING STOPWORDS\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\W+', ' ', text)  # Remove non-word characters\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Apply stemming and lemmatization, remove stopwords\n",
        "    tokens = [lemmatizer.lemmatize(stemmer.stem(token)) for token in tokens if token not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def build_index(corpus_path):\n",
        "    index = defaultdict(list)\n",
        "    doc_lengths = {}\n",
        "    N = 0  # Total number of documents\n",
        "    doc_ids = {}\n",
        "\n",
        "    for i, filename in enumerate(os.listdir(corpus_path)):\n",
        "        if filename.endswith('.txt'):\n",
        "            N += 1\n",
        "            doc_id = i + 1\n",
        "            doc_ids[filename] = doc_id\n",
        "            with open(os.path.join(corpus_path, filename), 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "                tokens = preprocess(content)\n",
        "                token_counts = Counter(tokens)\n",
        "\n",
        "                # Add terms to the index\n",
        "                for term, freq in token_counts.items():\n",
        "                    index[term].append((doc_id, freq))\n",
        "\n",
        "                # Compute the document length (for normalization later, lnc scheme: log(tf) and normalize)\n",
        "                length = 0\n",
        "                for term, freq in token_counts.items():\n",
        "                    length += (1 + math.log10(freq))**2\n",
        "                doc_lengths[doc_id] = math.sqrt(length)\n",
        "\n",
        "    return index, doc_lengths, N, doc_ids\n",
        "\n",
        "# Calculating cosine similarity between the query and documents\n",
        "#WROTE CODE ACCORDING TO FUNCTION GIVEN IN SLIDES\n",
        "# COSINESCORE(q)\n",
        "# 1 float Scores[N] = 0\n",
        "# 2 float Length[N]\n",
        "# 3 **for** each query term t\n",
        "# 4 **do** calculate wt,q and fetch postings list for t\n",
        "# 5 **for** each pair(d, tft,d) in postings list\n",
        "# 6 **do** Scores[d]+ = Wt,d × Wt,q\n",
        "# 7 Read the array Length\n",
        "# 8 **for** each d\n",
        "# 9 **do** Scores[d] = Scores[d]/Length[d]\n",
        "# 10 **return** Top K components of Scores[]\n",
        "def cosine_similarity(query_vec, doc_vec, doc_lengths):\n",
        "    scores = defaultdict(float)\n",
        "    for term, qtf_idf in query_vec.items():\n",
        "        if term in doc_vec:\n",
        "            for doc_id, dtf_idf in doc_vec[term]:\n",
        "                scores[doc_id] += qtf_idf * dtf_idf\n",
        "\n",
        "    for doc_id in scores:\n",
        "        scores[doc_id] /= doc_lengths[doc_id]  # Normalize by document length\n",
        "\n",
        "    return sorted(scores.items(), key=lambda x: (-x[1], x[0]))\n",
        "\n",
        "# Vector space model with lnc.ltc ranking\n",
        "def vsm_search(query, index, doc_lengths, N, doc_ids):\n",
        "    # the query is preprocessed so that lnc.ltn similarity can be calculated appropriately\n",
        "    query_tokens = preprocess(query)\n",
        "    query_freqs = Counter(query_tokens)\n",
        "\n",
        "    # Compute tf-idf for query using ltc (log tf, idf, no normalization)\n",
        "    query_vec = {}\n",
        "    for term, freq in query_freqs.items():\n",
        "        if term in index:\n",
        "            df = len(index[term])  # Document frequency\n",
        "            idf = math.log10(N / df)\n",
        "            query_vec[term] = (1 + math.log10(freq)) * idf\n",
        "\n",
        "    # Compute tf for documents using lnc (log tf, no idf, normalized by length)\n",
        "    doc_vec = defaultdict(list)\n",
        "    for term, postings in index.items():\n",
        "        for doc_id, freq in postings:\n",
        "            tf = (1 + math.log10(freq))  # No idf, just log(tf)\n",
        "            doc_vec[term].append((doc_id, tf))\n",
        "\n",
        "    # Rank documents by cosine similarity\n",
        "    ranked_docs = cosine_similarity(query_vec, doc_vec, doc_lengths)\n",
        "\n",
        "    # Convert doc IDs back to filenames and return top 10 results\n",
        "    ranked_files = [(list(doc_ids.keys())[list(doc_ids.values()).index(doc_id)], score) for doc_id, score in ranked_docs[:10]]\n",
        "    return ranked_files\n",
        "\n",
        "# Main function to run the search engine\n",
        "def main():\n",
        "    corpus_path = 'corpus'\n",
        "    index, doc_lengths, N, doc_ids = build_index(corpus_path)\n",
        "\n",
        "    while True:\n",
        "        query = input(\"Enter your query (or 'exit' to quit): \").strip()\n",
        "        if query.lower() == 'exit':\n",
        "            break\n",
        "        results = vsm_search(query, index, doc_lengths, N, doc_ids)\n",
        "        if results:\n",
        "            print(\"\\nTop results:\")\n",
        "            for i, (filename, score) in enumerate(results, 1):\n",
        "                print(f\"{i}. {filename} (Score: {score})\")\n",
        "        else:\n",
        "            print(\"No documents match the query.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YXujYAng8Fx8",
        "outputId": "46b0031a-ade2-4c26-932d-1e3fbf70cac0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query (or 'exit' to quit): Developing your Zomato business account and profile is a great way to boost your restaurant’s online reputation\n",
            "\n",
            "Top results:\n",
            "1. zomato.txt (Score: 0.5488501501085257)\n",
            "2. swiggy.txt (Score: 0.3351914447855742)\n",
            "3. instagram.txt (Score: 0.15470647490300268)\n",
            "4. messenger.txt (Score: 0.1512386655130801)\n",
            "5. youtube.txt (Score: 0.14940567205185346)\n",
            "6. Discord.txt (Score: 0.1364887404785657)\n",
            "7. bing.txt (Score: 0.13235296907216795)\n",
            "8. paypal.txt (Score: 0.12035497736893036)\n",
            "9. reddit.txt (Score: 0.11274273968889387)\n",
            "10. flipkart.txt (Score: 0.10410504117596259)\n",
            "Enter your query (or 'exit' to quit): Warwickshire, came from an ancient family and was the heiress to some land\n",
            "\n",
            "Top results:\n",
            "1. shakespeare.txt (Score: 0.37902335875841886)\n",
            "2. levis.txt (Score: 0.07626953861676306)\n",
            "3. Adobe.txt (Score: 0.07155668757671003)\n",
            "4. google.txt (Score: 0.06551292559905736)\n",
            "5. nike.txt (Score: 0.06069062772402075)\n",
            "6. zomato.txt (Score: 0.05595827061016485)\n",
            "7. huawei.txt (Score: 0.04335732544708555)\n",
            "8. skype.txt (Score: 0.037033682061923257)\n",
            "9. blackberry.txt (Score: 0.03451830379344207)\n",
            "10. Dell.txt (Score: 0.034012569930975914)\n",
            "Enter your query (or 'exit' to quit): company was founded in 1982 by John Warnock and Charles Geschke. While employed at Xerox Corporation’s Palo Alto (California) Research Center (PARC), the two computer scientists had developed a programming language specially designed to describe the precise position, shape, and size of objects on a computer-generated page. This page description language, later known as PostScript, described such objects as letters and graphics in mathematical terms, without reference to any specific computer or printer; any device capable of interpreting the language would be able to generate a representation of the page at any resolution the device supported. When Xerox declined to bring the technology to market, Warnock and Geschke formed their own company to do so, naming it after a creek near their homes.  In 1983 Apple Computer, Inc\n",
            "\n",
            "Top results:\n",
            "1. Adobe.txt (Score: 3.2383822774861453)\n",
            "2. HP.txt (Score: 0.8246008977138998)\n",
            "3. microsoft.txt (Score: 0.5803847812114424)\n",
            "4. google.txt (Score: 0.568448599287142)\n",
            "5. motorola.txt (Score: 0.5463963458073737)\n",
            "6. shakespeare.txt (Score: 0.5348250959967221)\n",
            "7. Amazon.txt (Score: 0.5053584975446278)\n",
            "8. Lenovo.txt (Score: 0.49759571762890015)\n",
            "9. operating.txt (Score: 0.4943773659859472)\n",
            "10. sony.txt (Score: 0.4891928263596391)\n",
            "Enter your query (or 'exit' to quit): The following year Parker and entrepreneurs Minh Nguyen, Todd Masonis, and Cameron Ring founded Plaxo, a Web site that hosted a downloadable software application that served as an online address book for users to collect contact information. Parker was fired from Plaxo in 2004 for his erratic engagement with the company. Interested in the possibilities of social networking, he was intrigued by Thefacebook (later to become Facebook), a social networking Web site for college students cofounded by Harvard University student Mark Zuckerberg. Parker encouraged Zuckerberg to drop out of Harvard to devote himself to the social network and helped negotiate financing for Facebook from Paypal cofounder Peter Thiel and the venture capital firm Accel Partners. In securing the financing for Facebook, Parker was able to stipulate that Zuckerberg would retain majority control over Facebook’s board of directors. Parker became president of Facebook in 2004\n",
            "\n",
            "Top results:\n",
            "1. spotify.txt (Score: 5.0339127868570435)\n",
            "2. Dell.txt (Score: 0.6853546248946437)\n",
            "3. google.txt (Score: 0.6723717786950337)\n",
            "4. telegram.txt (Score: 0.6456828732080216)\n",
            "5. messenger.txt (Score: 0.6380145439973541)\n",
            "6. zomato.txt (Score: 0.6288159027422929)\n",
            "7. Adobe.txt (Score: 0.5653276599452145)\n",
            "8. reliance.txt (Score: 0.5527639789260679)\n",
            "9. microsoft.txt (Score: 0.5500329875376281)\n",
            "10. levis.txt (Score: 0.5491976900328678)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-50cd30d927b0>\u001b[0m in \u001b[0;36m<cell line: 116>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-50cd30d927b0>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your query (or 'exit' to quit): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rahuldj2/IR-Assignment-2/blob/main/IR_ASSN_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YXujYAng8Fx8",
        "outputId": "46b0031a-ade2-4c26-932d-1e3fbf70cac0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\punya\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\punya\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\punya\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No documents match the query.\n",
            "No documents match the query.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# for preprocessing steps initializing stemming, lemmatizers etc\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "#IN THIS STEP TEXT IS FIRST CONVERTED TO LOWERCASE THEN LEMMATIZATION IS APPLIED AFTER REMOVING STOPWORDS\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\W+', ' ', text)  # Remove non-word characters\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Apply stemming and lemmatization, remove stopwords\n",
        "    tokens = [lemmatizer.lemmatize(stemmer.stem(token)) for token in tokens if token not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def build_index(corpus_path):\n",
        "    index = defaultdict(list)\n",
        "    doc_lengths = {}\n",
        "    N = 0  # Total number of documents\n",
        "    doc_ids = {}\n",
        "\n",
        "    for i, filename in enumerate(os.listdir(corpus_path)):\n",
        "        if filename.endswith('.txt'):\n",
        "            N += 1\n",
        "            doc_id = i + 1\n",
        "            doc_ids[filename] = doc_id\n",
        "            with open(os.path.join(corpus_path, filename), 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "                tokens = preprocess(content)\n",
        "                token_counts = Counter(tokens)\n",
        "\n",
        "                # Add terms to the index\n",
        "                for term, freq in token_counts.items():\n",
        "                    index[term].append((doc_id, freq))\n",
        "\n",
        "                # Compute the document length (for normalization later, lnc scheme: log(tf) and normalize)\n",
        "                length = 0\n",
        "                for term, freq in token_counts.items():\n",
        "                    length += (1 + math.log10(freq))**2\n",
        "                doc_lengths[doc_id] = math.sqrt(length)\n",
        "\n",
        "    return index, doc_lengths, N, doc_ids\n",
        "\n",
        "# Calculating cosine similarity between the query and documents\n",
        "#WROTE CODE ACCORDING TO FUNCTION GIVEN IN SLIDES\n",
        "# COSINESCORE(q)\n",
        "# 1 float Scores[N] = 0\n",
        "# 2 float Length[N]\n",
        "# 3 **for** each query term t\n",
        "# 4 **do** calculate wt,q and fetch postings list for t\n",
        "# 5 **for** each pair(d, tft,d) in postings list\n",
        "# 6 **do** Scores[d]+ = Wt,d × Wt,q\n",
        "# 7 Read the array Length\n",
        "# 8 **for** each d\n",
        "# 9 **do** Scores[d] = Scores[d]/Length[d]\n",
        "# 10 **return** Top K components of Scores[]\n",
        "def cosine_similarity(query_vec, doc_vec, doc_lengths):\n",
        "    scores = defaultdict(float)\n",
        "    for term, qtf_idf in query_vec.items():\n",
        "        if term in doc_vec:\n",
        "            for doc_id, dtf_idf in doc_vec[term]:\n",
        "                scores[doc_id] += qtf_idf * dtf_idf\n",
        "\n",
        "    for doc_id in scores:\n",
        "        scores[doc_id] /= doc_lengths[doc_id]  # Normalize by document length\n",
        "\n",
        "    return sorted(scores.items(), key=lambda x: (-x[1], x[0]))\n",
        "\n",
        "# Vector space model with lnc.ltc ranking\n",
        "def vsm_search(query, index, doc_lengths, N, doc_ids):\n",
        "    # the query is preprocessed so that lnc.ltn similarity can be calculated appropriately\n",
        "    query_tokens = preprocess(query)\n",
        "    query_freqs = Counter(query_tokens)\n",
        "\n",
        "    # Compute tf-idf for query using ltc (log tf, idf, no normalization)\n",
        "    query_vec = {}\n",
        "    for term, freq in query_freqs.items():\n",
        "        if term in index:\n",
        "            df = len(index[term])  # Document frequency\n",
        "            idf = math.log10(N / df)\n",
        "            query_vec[term] = (1 + math.log10(freq)) * idf\n",
        "\n",
        "    # Compute tf for documents using lnc (log tf, no idf, normalized by length)\n",
        "    doc_vec = defaultdict(list)\n",
        "    for term, postings in index.items():\n",
        "        for doc_id, freq in postings:\n",
        "            tf = (1 + math.log10(freq))  # No idf, just log(tf)\n",
        "            doc_vec[term].append((doc_id, tf))\n",
        "\n",
        "    # Rank documents by cosine similarity\n",
        "    ranked_docs = cosine_similarity(query_vec, doc_vec, doc_lengths)\n",
        "\n",
        "    # Convert doc IDs back to filenames and return top 10 results\n",
        "    ranked_files = [(list(doc_ids.keys())[list(doc_ids.values()).index(doc_id)], score) for doc_id, score in ranked_docs[:10]]\n",
        "    return ranked_files\n",
        "\n",
        "# Main function to run the search engine\n",
        "def main():\n",
        "    corpus_path = 'corpus'\n",
        "    index, doc_lengths, N, doc_ids = build_index(corpus_path)\n",
        "\n",
        "    while True:\n",
        "        query = input(\"Enter your query (or 'exit' to quit): \").strip()\n",
        "        if query.lower() == 'exit':\n",
        "            break\n",
        "        results = vsm_search(query, index, doc_lengths, N, doc_ids)\n",
        "        if results:\n",
        "            print(\"\\nTop results:\")\n",
        "            for i, (filename, score) in enumerate(results, 1):\n",
        "                print(f\"{i}. {filename} (Score: {score})\")\n",
        "        else:\n",
        "            print(\"No documents match the query.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\punya\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\punya\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\punya\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Results for query: 'Developing your Zomato business account and profile is a great way to boost your restaurant’s online reputation'\n",
            "1. zomato.txt (Score: 0.5293531487992237)\n",
            "2. swiggy.txt (Score: 0.3175398730755558)\n",
            "3. instagram.txt (Score: 0.14883350305333537)\n",
            "4. messenger.txt (Score: 0.14665081801495392)\n",
            "5. youtube.txt (Score: 0.14511951736250497)\n",
            "6. Discord.txt (Score: 0.13150818490012786)\n",
            "7. bing.txt (Score: 0.12748523022536348)\n",
            "8. paypal.txt (Score: 0.11895867761593869)\n",
            "9. reddit.txt (Score: 0.10875265715819195)\n",
            "10. flipkart.txt (Score: 0.10235843650392648)\n",
            "\n",
            "Results for query: 'Warwickshire, came from an ancient family and was the heiress to some land'\n",
            "1. shakespeare.txt (Score: 0.37145786629278843)\n",
            "2. levis.txt (Score: 0.07454142776109814)\n",
            "3. Adobe.txt (Score: 0.07034525589618056)\n",
            "4. google.txt (Score: 0.06421724189263736)\n",
            "5. nike.txt (Score: 0.0582325631093714)\n",
            "6. zomato.txt (Score: 0.05397044483451929)\n",
            "7. huawei.txt (Score: 0.042458692963560585)\n",
            "8. skype.txt (Score: 0.035918342197699515)\n",
            "9. blackberry.txt (Score: 0.03403985589531049)\n",
            "10. Dell.txt (Score: 0.03344242290504274)\n",
            "\n",
            "Results for query: 'Their Design School, filled with free video courses on a wide range of topics, is a good place to start. There's a '\n",
            "1. canva.txt (Score: 0.5915231052355223)\n",
            "2. Discord.txt (Score: 0.28541017314163025)\n",
            "3. steam.txt (Score: 0.28332803914413623)\n",
            "4. youtube.txt (Score: 0.18238882054888333)\n",
            "5. shakespeare.txt (Score: 0.17956754091877078)\n",
            "6. yahoo.txt (Score: 0.16509172157618884)\n",
            "7. Lenovo.txt (Score: 0.16060899468019027)\n",
            "8. skype.txt (Score: 0.14490678195121492)\n",
            "9. blackberry.txt (Score: 0.14338038201034836)\n",
            "10. microsoft.txt (Score: 0.14334039773708113)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "# for preprocessing steps initializing stemming, lemmatizers etc\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "\n",
        "# IN THIS STEP TEXT IS FIRST CONVERTED TO LOWERCASE THEN LEMMATIZATION IS APPLIED AFTER REMOVING STOPWORDS\n",
        "def preprocess(text):\n",
        "    # text = text.lower()\n",
        "    text = re.sub(r\"\\W+\", \" \", text)  # Remove non-word characters\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Apply stemming and lemmatization, remove stopwords\n",
        "    tokens = [\n",
        "        lemmatizer.lemmatize(stemmer.stem(token))\n",
        "        for token in tokens\n",
        "        if token not in stop_words\n",
        "    ]\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def build_index(corpus_path):\n",
        "    index = defaultdict(list)\n",
        "    doc_lengths = {}\n",
        "    N = 0  # Total number of documents\n",
        "    doc_ids = {}\n",
        "\n",
        "    for i, filename in enumerate(os.listdir(corpus_path)):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            N += 1\n",
        "            doc_id = i + 1\n",
        "            doc_ids[filename] = doc_id\n",
        "            with open(os.path.join(corpus_path, filename), \"r\", encoding=\"utf-8\") as f:\n",
        "                content = f.read()\n",
        "                tokens = preprocess(content)\n",
        "                token_counts = Counter(tokens)\n",
        "\n",
        "                # Add terms to the index\n",
        "                for term, freq in token_counts.items():\n",
        "                    index[term].append((doc_id, freq))\n",
        "\n",
        "                # Compute the document length (for normalization later, lnc scheme: log(tf) and normalize)\n",
        "                length = 0\n",
        "                for term, freq in token_counts.items():\n",
        "                    length += (1 + math.log10(freq)) ** 2\n",
        "                doc_lengths[doc_id] = math.sqrt(length)\n",
        "\n",
        "    return index, doc_lengths, N, doc_ids\n",
        "\n",
        "\n",
        "# Calculating cosine similarity between the query and documents\n",
        "# WROTE CODE ACCORDING TO FUNCTION GIVEN IN SLIDES\n",
        "# COSINESCORE(q)\n",
        "# 1 float Scores[N] = 0\n",
        "# 2 float Length[N]\n",
        "# 3 **for** each query term t\n",
        "# 4 **do** calculate wt,q and fetch postings list for t\n",
        "# 5 **for** each pair(d, tft,d) in postings list\n",
        "# 6 **do** Scores[d]+ = Wt,d × Wt,q\n",
        "# 7 Read the array Length\n",
        "# 8 **for** each d\n",
        "# 9 **do** Scores[d] = Scores[d]/Length[d]\n",
        "# 10 **return** Top K components of Scores[]\n",
        "def cosine_similarity(query_vec, doc_vec, doc_lengths):\n",
        "    scores = defaultdict(float)\n",
        "    for term, qtf_idf in query_vec.items():\n",
        "        if term in doc_vec:\n",
        "            for doc_id, dtf_idf in doc_vec[term]:\n",
        "                scores[doc_id] += qtf_idf * dtf_idf\n",
        "\n",
        "    for doc_id in scores:\n",
        "        scores[doc_id] /= doc_lengths[doc_id]  # Normalize by document length\n",
        "\n",
        "    return sorted(scores.items(), key=lambda x: (-x[1], x[0]))\n",
        "\n",
        "\n",
        "# Vector space model with lnc.ltc ranking\n",
        "def vsm_search(query, index, doc_lengths, N, doc_ids):\n",
        "    # the query is preprocessed so that lnc.ltn similarity can be calculated appropriately\n",
        "    query_tokens = preprocess(query)\n",
        "    query_freqs = Counter(query_tokens)\n",
        "\n",
        "    # Compute tf-idf for query using ltc (log tf, idf, no normalization)\n",
        "    query_vec = {}\n",
        "    for term, freq in query_freqs.items():\n",
        "        if term in index:\n",
        "            df = len(index[term])  # Document frequency\n",
        "            idf = math.log10(N / df)\n",
        "            query_vec[term] = (1 + math.log10(freq)) * idf\n",
        "\n",
        "    # Compute tf for documents using lnc (log tf, no idf, normalized by length)\n",
        "    doc_vec = defaultdict(list)\n",
        "    for term, postings in index.items():\n",
        "        for doc_id, freq in postings:\n",
        "            tf = 1 + math.log10(freq)  # No idf, just log(tf)\n",
        "            doc_vec[term].append((doc_id, tf))\n",
        "\n",
        "    # Rank documents by cosine similarity\n",
        "    ranked_docs = cosine_similarity(query_vec, doc_vec, doc_lengths)\n",
        "\n",
        "    # Convert doc IDs back to filenames and return top 10 results\n",
        "    ranked_files = [\n",
        "        (list(doc_ids.keys())[list(doc_ids.values()).index(doc_id)], score)\n",
        "        for doc_id, score in ranked_docs[:10]\n",
        "    ]\n",
        "    return ranked_files\n",
        "\n",
        "\n",
        "# Main function to run the search engine\n",
        "def main():\n",
        "    corpus_path = \"corpus\"\n",
        "    index, doc_lengths, N, doc_ids = build_index(corpus_path)\n",
        "\n",
        "    queries = [\n",
        "        \"Developing your Zomato business account and profile is a great way to boost your restaurant’s online reputation\",\n",
        "        \"Warwickshire, came from an ancient family and was the heiress to some land\",\n",
        "        \"Their Design School, filled with free video courses on a wide range of topics, is a good place to start. There's a \",\n",
        "    ]\n",
        "\n",
        "    for query in queries:\n",
        "        results = vsm_search(query, index, doc_lengths, N, doc_ids)\n",
        "        if results:\n",
        "            print(f\"\\nResults for query: '{query}'\")\n",
        "            for i, (filename, score) in enumerate(results, 1):\n",
        "                print(f\"{i}. {filename} (Score: {score})\")\n",
        "        else:\n",
        "            print(f\"No documents match the query: '{query}'\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMuTfGEtHxOyqwB6pa8fYO2",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

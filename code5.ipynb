{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\punya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\punya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\punya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: Developing your Zomato business account and profile is a great way to boost your restaurant’s online reputation\n",
      "\n",
      "Top results:\n",
      "1. zomato.txt (Score: 0.214723)\n",
      "2. swiggy.txt (Score: 0.131135)\n",
      "3. instagram.txt (Score: 0.060525)\n",
      "4. messenger.txt (Score: 0.059168)\n",
      "5. youtube.txt (Score: 0.058451)\n",
      "6. Discord.txt (Score: 0.053398)\n",
      "7. bing.txt (Score: 0.051780)\n",
      "8. paypal.txt (Score: 0.047086)\n",
      "9. reddit.txt (Score: 0.044108)\n",
      "10. flipkart.txt (Score: 0.040728)\n",
      "--------------------------------------------------\n",
      "Query 2: Warwickshire, came from an ancient family and was the heiress to some land\n",
      "\n",
      "Top results:\n",
      "1. shakespeare.txt (Score: 0.119976)\n",
      "2. levis.txt (Score: 0.024142)\n",
      "3. Adobe.txt (Score: 0.022651)\n",
      "4. google.txt (Score: 0.020737)\n",
      "5. nike.txt (Score: 0.019211)\n",
      "6. zomato.txt (Score: 0.017713)\n",
      "7. huawei.txt (Score: 0.013724)\n",
      "8. skype.txt (Score: 0.011723)\n",
      "9. blackberry.txt (Score: 0.010926)\n",
      "10. Dell.txt (Score: 0.010766)\n",
      "--------------------------------------------------\n",
      "Query 3: Their Design School, filled with free video courses on a wide range of topics, is a good place to start. There's a \n",
      "\n",
      "Top results:\n",
      "1. canva.txt (Score: 0.225888)\n",
      "2. steam.txt (Score: 0.133305)\n",
      "3. Discord.txt (Score: 0.122922)\n",
      "4. youtube.txt (Score: 0.077225)\n",
      "5. shakespeare.txt (Score: 0.071409)\n",
      "6. yahoo.txt (Score: 0.067628)\n",
      "7. microsoft.txt (Score: 0.066956)\n",
      "8. instagram.txt (Score: 0.063315)\n",
      "9. blackberry.txt (Score: 0.058173)\n",
      "10. skype.txt (Score: 0.057971)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Initialize stemmer, lemmatizer, and stop words\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Preprocessing function: lowercase, remove punctuation, remove stopwords, apply stemming and lemmatization\n",
    "def preprocess(text):\n",
    "    text = text.lower()  # case folding\n",
    "    text = re.sub(r\"\\W+\", \" \", text)  # remove non-word characters\n",
    "    tokens = text.split()\n",
    "    tokens = [\n",
    "        lemmatizer.lemmatize(stemmer.stem(token))\n",
    "        for token in tokens\n",
    "        if token not in stop_words\n",
    "    ]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def build_index(corpus_dir):\n",
    "    index = defaultdict(list)  # term -> postings (docID, tf_weight)\n",
    "    doc_lengths = {}  # docID -> document length (for normalization)\n",
    "    N = 0  # total number of documents\n",
    "    doc_ids = {}  # mapping of filenames to docIDs\n",
    "\n",
    "    for docID, filename in enumerate(os.listdir(corpus_dir), 1):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            N += 1\n",
    "            doc_ids[docID] = filename\n",
    "            with open(\n",
    "                os.path.join(corpus_dir, filename), \"r\", encoding=\"utf-8\"\n",
    "            ) as file:\n",
    "                content = file.read()\n",
    "                tokens = preprocess(content)\n",
    "                term_freqs = Counter(tokens)\n",
    "                tf_weights = {}\n",
    "                doc_length = 0\n",
    "\n",
    "                for term, freq in term_freqs.items():\n",
    "                    tf_weight = 1 + math.log10(freq)  # lnc: 1 + log10(tf)\n",
    "                    tf_weights[term] = tf_weight\n",
    "                    doc_length += tf_weight**2  # sum of squares of weights\n",
    "\n",
    "                doc_length = math.sqrt(doc_length)  # document length for normalization\n",
    "\n",
    "                # Store raw tf_weight in the index (no normalization here)\n",
    "                for term, tf_weight in tf_weights.items():\n",
    "                    index[term].append((docID, tf_weight))\n",
    "\n",
    "                doc_lengths[docID] = doc_length\n",
    "\n",
    "    return index, doc_lengths, N, doc_ids\n",
    "\n",
    "# Calculate the tf-idf for the query using ltc for queries\n",
    "def compute_query_tfidf(query, index, N):\n",
    "    query_tokens = preprocess(query)\n",
    "    query_term_freqs = Counter(query_tokens)\n",
    "    query_tfidf = {}\n",
    "    query_length = 0  # sum of squares of term tf-idf weights\n",
    "\n",
    "    for term, freq in query_term_freqs.items():\n",
    "        if term in index:\n",
    "            df = len(index[term])  # document frequency\n",
    "            idf = math.log10(N / df)  # idf = log10(N/df)\n",
    "            tf_weight = 1 + math.log10(freq)  # ltc: 1 + log10(tf) for query\n",
    "            tf_idf = tf_weight * idf  # tf-idf = tf_weight * idf\n",
    "            query_tfidf[term] = tf_idf\n",
    "            query_length += tf_idf**2  # sum of squares of tf-idf weights\n",
    "\n",
    "    query_length = math.sqrt(query_length)  # normalize query length\n",
    "\n",
    "    # Normalize the query's tf-idf weights\n",
    "    for term in query_tfidf:\n",
    "        query_tfidf[term] /= query_length\n",
    "\n",
    "    return query_tfidf, query_length\n",
    "\n",
    "\n",
    "def cosine_similarity(query_tfidf, query_length, index, doc_lengths):\n",
    "    doc_scores = defaultdict(float)\n",
    "\n",
    "    for term, query_weight in query_tfidf.items():\n",
    "        if term in index:\n",
    "            for docID, doc_weight in index[term]:\n",
    "                doc_scores[docID] += query_weight * doc_weight\n",
    "\n",
    "    # Normalize by document lengths (keep this normalization)\n",
    "    for docID in doc_scores:\n",
    "        doc_scores[docID] /= doc_lengths[docID]\n",
    "\n",
    "    return sorted(doc_scores.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "\n",
    "# Search function to process queries and return top 10 relevant documents\n",
    "def search(query, index, doc_lengths, N, doc_ids):\n",
    "    query_tfidf, query_length = compute_query_tfidf(query, index, N)\n",
    "    ranked_docs = cosine_similarity(query_tfidf, query_length, index, doc_lengths)\n",
    "\n",
    "    # Map docIDs back to filenames and return top 10 results\n",
    "    ranked_files = [(doc_ids[docID], score) for docID, score in ranked_docs[:10]]\n",
    "    return ranked_files\n",
    "\n",
    "\n",
    "# Load and index the corpus\n",
    "corpus_dir = \"corpus\"  # Directory containing your corpus of text files\n",
    "queries = [\n",
    "    \"Developing your Zomato business account and profile is a great way to boost your restaurant’s online reputation\",\n",
    "    \"Warwickshire, came from an ancient family and was the heiress to some land\",\n",
    "    \"Their Design School, filled with free video courses on a wide range of topics, is a good place to start. There's a \",\n",
    "]\n",
    "index, doc_lengths, N, doc_ids = build_index(corpus_dir)\n",
    "\n",
    "# Test queries\n",
    "for i, query in enumerate(queries, 1):\n",
    "    print(f\"Query {i}: {query}\")\n",
    "    results = search(query, index, doc_lengths, N, doc_ids)\n",
    "    if results:\n",
    "        print(\"\\nTop results:\")\n",
    "        for rank, (filename, score) in enumerate(results, 1):\n",
    "            print(f\"{rank}. {filename} (Score: {score:.6f})\")\n",
    "    else:\n",
    "        print(\"No documents match the query.\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\punya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\punya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\punya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: Developing your Zomato business account and profile is a great way to boost your restaurant’s online reputation\n",
      "\n",
      "Top results:\n",
      "1. zomato.txt (Score: 0.214723)\n",
      "2. swiggy.txt (Score: 0.131135)\n",
      "3. instagram.txt (Score: 0.060525)\n",
      "4. messenger.txt (Score: 0.059168)\n",
      "5. youtube.txt (Score: 0.058451)\n",
      "6. Discord.txt (Score: 0.053398)\n",
      "7. bing.txt (Score: 0.051780)\n",
      "8. paypal.txt (Score: 0.047086)\n",
      "9. reddit.txt (Score: 0.044108)\n",
      "10. flipkart.txt (Score: 0.040728)\n",
      "--------------------------------------------------\n",
      "Query 2: Warwickshire, came from an ancient family and was the heiress to some land\n",
      "\n",
      "Top results:\n",
      "1. shakespeare.txt (Score: 0.119976)\n",
      "2. levis.txt (Score: 0.024142)\n",
      "3. Adobe.txt (Score: 0.022651)\n",
      "4. google.txt (Score: 0.020737)\n",
      "5. nike.txt (Score: 0.019211)\n",
      "6. zomato.txt (Score: 0.017713)\n",
      "7. huawei.txt (Score: 0.013724)\n",
      "8. skype.txt (Score: 0.011723)\n",
      "9. blackberry.txt (Score: 0.010926)\n",
      "10. Dell.txt (Score: 0.010766)\n",
      "--------------------------------------------------\n",
      "Query 3: Their Design School, filled with free video courses on a wide range of topics, is a good place to start. There's a \n",
      "\n",
      "Top results:\n",
      "1. canva.txt (Score: 0.225888)\n",
      "2. steam.txt (Score: 0.133305)\n",
      "3. Discord.txt (Score: 0.122922)\n",
      "4. youtube.txt (Score: 0.077225)\n",
      "5. shakespeare.txt (Score: 0.071409)\n",
      "6. yahoo.txt (Score: 0.067628)\n",
      "7. microsoft.txt (Score: 0.066956)\n",
      "8. instagram.txt (Score: 0.063315)\n",
      "9. blackberry.txt (Score: 0.058173)\n",
      "10. skype.txt (Score: 0.057971)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Initialize stemmer, lemmatizer, and stop words\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Preprocessing function: lowercase, remove punctuation, remove stopwords, apply stemming and lemmatization\n",
    "def preprocess(text):\n",
    "    text = text.lower()  # case folding\n",
    "    text = re.sub(r\"\\W+\", \" \", text)  # remove non-word characters\n",
    "    tokens = text.split()\n",
    "    tokens = [\n",
    "        lemmatizer.lemmatize(stemmer.stem(token))\n",
    "        for token in tokens\n",
    "        if token not in stop_words\n",
    "    ]\n",
    "    return tokens\n",
    "\n",
    "def build_index(corpus_dir):\n",
    "    index = defaultdict(list)  # term -> postings (docID, tf_weight)\n",
    "    doc_lengths = {}  # docID -> document length (for normalization)\n",
    "    N = 0  # total number of documents\n",
    "    doc_ids = {}  # mapping of filenames to docIDs\n",
    "\n",
    "    for docID, filename in enumerate(os.listdir(corpus_dir), 1):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            N += 1\n",
    "            doc_ids[docID] = filename\n",
    "            with open(\n",
    "                os.path.join(corpus_dir, filename), \"r\", encoding=\"utf-8\"\n",
    "            ) as file:\n",
    "                content = file.read()\n",
    "                tokens = preprocess(content)\n",
    "                term_freqs = Counter(tokens)\n",
    "                tf_weights = {}\n",
    "                doc_length = 0\n",
    "\n",
    "                for term, freq in term_freqs.items():\n",
    "                    tf_weight = 1 + math.log10(freq)  # lnc: 1 + log10(tf)\n",
    "                    tf_weights[term] = tf_weight\n",
    "                    doc_length += tf_weight**2  # sum of squares of weights\n",
    "\n",
    "                doc_length = math.sqrt(doc_length)  # document length for normalization\n",
    "\n",
    "                # Store raw tf_weight in the index (no normalization here)\n",
    "                for term, tf_weight in tf_weights.items():\n",
    "                    index[term].append((docID, tf_weight))\n",
    "\n",
    "                doc_lengths[docID] = doc_length\n",
    "\n",
    "    return index, doc_lengths, N, doc_ids\n",
    "\n",
    "# Calculate the tf-idf for the query using ltn for queries\n",
    "def compute_query_tfidf(query, index, N):\n",
    "    query_tokens = preprocess(query)\n",
    "    query_term_freqs = Counter(query_tokens)\n",
    "    query_tfidf = {}\n",
    "    query_length = 0  # sum of squares of term tf-idf weights\n",
    "\n",
    "    for term, freq in query_term_freqs.items():\n",
    "        if term in index:\n",
    "            df = len(index[term])  # document frequency\n",
    "            idf = math.log10(N / df)  # idf = log10(N/df)\n",
    "            tf_weight = 1 + math.log10(freq)  # ltn: 1 + log10(tf) for query\n",
    "            tf_idf = tf_weight * idf  # tf-idf = tf_weight * idf\n",
    "            query_tfidf[term] = tf_idf\n",
    "            query_length += tf_idf**2  # sum of squares of tf-idf weights\n",
    "\n",
    "    query_length = math.sqrt(query_length)  # normalize query length\n",
    "\n",
    "    # Normalize the query's tf-idf weights\n",
    "    for term in query_tfidf:\n",
    "        query_tfidf[term] /= query_length\n",
    "\n",
    "    return query_tfidf, query_length\n",
    "\n",
    "def cosine_similarity(query_tfidf, query_length, index, doc_lengths):\n",
    "    doc_scores = defaultdict(float)\n",
    "\n",
    "    for term, query_weight in query_tfidf.items():\n",
    "        if term in index:\n",
    "            for docID, doc_weight in index[term]:\n",
    "                doc_scores[docID] += query_weight * doc_weight\n",
    "\n",
    "    # Normalize by document lengths (keep this normalization)\n",
    "    for docID in doc_scores:\n",
    "        doc_scores[docID] /= doc_lengths[docID]\n",
    "\n",
    "    return sorted(doc_scores.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "# Search function to process queries and return top 10 relevant documents\n",
    "def search(query, index, doc_lengths, N, doc_ids):\n",
    "    query_tfidf, query_length = compute_query_tfidf(query, index, N)\n",
    "    ranked_docs = cosine_similarity(query_tfidf, query_length, index, doc_lengths)\n",
    "\n",
    "    # Map docIDs back to filenames and return top 10 results\n",
    "    ranked_files = [(doc_ids[docID], score) for docID, score in ranked_docs[:10]]\n",
    "    return ranked_files\n",
    "\n",
    "# Load and index the corpus\n",
    "corpus_dir = \"corpus\"  # Directory containing your corpus of text files\n",
    "queries = [\n",
    "    \"Developing your Zomato business account and profile is a great way to boost your restaurant’s online reputation\",\n",
    "    \"Warwickshire, came from an ancient family and was the heiress to some land\",\n",
    "    \"Their Design School, filled with free video courses on a wide range of topics, is a good place to start. There's a \",\n",
    "]\n",
    "index, doc_lengths, N, doc_ids = build_index(corpus_dir)\n",
    "\n",
    "# Test queries\n",
    "for i, query in enumerate(queries, 1):\n",
    "    print(f\"Query {i}: {query}\")\n",
    "    results = search(query, index, doc_lengths, N, doc_ids)\n",
    "    if results:\n",
    "        print(\"\\nTop results:\")\n",
    "        for rank, (filename, score) in enumerate(results, 1):\n",
    "            print(f\"{rank}. {filename} (Score: {score:.6f})\")\n",
    "    else:\n",
    "        print(\"No documents match the query.\")\n",
    "    print(\"-\" * 50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
